{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Y9HreIE5gxW1jsHze6qCZ1IKNCZCwI4X",
      "authorship_tag": "ABX9TyP24O9Oa7qzSrtdBFFHrm6t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DianaKahar/Data_management4/blob/main/P137263_Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**STQD6324 DATA MANAGEMENT**"
      ],
      "metadata": {
        "id": "YY7yZc5jJyws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark cassandra-driver pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DLZd5L6dwYb",
        "outputId": "f94bc155-4780-4c2a-ab63-29feacbe501f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cassandra-driver\n",
            "  Downloading cassandra_driver-3.29.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Collecting geomet<0.3,>=0.1 (from cassandra-driver)\n",
            "  Downloading geomet-0.2.1.post1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from geomet<0.3,>=0.1->cassandra-driver) (8.1.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from geomet<0.3,>=0.1->cassandra-driver) (1.16.0)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=2932d751dc44796719c75095f0c6adfe8c2656b3111336dd349f8732091bb744\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark, geomet, cassandra-driver\n",
            "Successfully installed cassandra-driver-3.29.1 geomet-0.2.1.post1 pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import avg, col, explode, array"
      ],
      "metadata": {
        "id": "NnRB9fukJxxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MovieLens Analysis\") \\\n",
        "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "4SFQrwQxJx0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_user(line):\n",
        "    fields = line.split('|')\n",
        "    return Row(user_id=int(fields[0]), age=int(fields[1]), gender=fields[2], occupation=fields[3], zip=fields[4])\n",
        "\n",
        "def parse_data(line):\n",
        "    fields = line.split(\"\\t\")\n",
        "    return Row(user_id=int(fields[0]), movie_id=int(fields[1]), rating=int(fields[2]), timestamp=int(fields[3]))\n",
        "\n",
        "def parse_item(line):\n",
        "    fields = line.split(\"|\")\n",
        "    genres = list(map(int, fields[5:]))\n",
        "    return Row(movie_id=int(fields[0]), title=fields[1], release_date=fields[2],\n",
        "               vid_release_date=fields[3], url=fields[4], genres=genres)"
      ],
      "metadata": {
        "id": "iUpH7SvrJx3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "    # Parse data\n",
        "    lines1 = spark.sparkContext.textFile(\"hdfs:///user/maria_dev/ml-100k/u.user\")\n",
        "    user = line1.map(parse_user)\n",
        "\n",
        "    line2 = spark.sparkContext.textFile(\"hdfs:///user/maria_dev/ml-100k/u.data\")\n",
        "    rating = line2.map(parse_data)\n",
        "\n",
        "    line3 = spark.sparkContext.textFile(\"hdfs:///user/maria_dev/ml-100k/u.item\")\n",
        "    name = line3.map(parse_item)\n",
        "\n",
        "    # Convert to Dataframe\n",
        "    userDT = spark.createDataFrame(user)\n",
        "    ratingDT = spark.createDataFrame(rating)\n",
        "    nameDT = spark.createDataFrame(name)\n",
        "\n",
        "    # Write DataFrames to Cassandra\n",
        "    userDT.write \\\n",
        "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
        "        .mode('append') \\\n",
        "        .options(table=\"user\", keyspace=\"movielen\") \\\n",
        "        .save()\n",
        "\n",
        "    ratingDT.write \\\n",
        "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
        "        .mode('append') \\\n",
        "        .options(table=\"rating\", keyspace=\"movielen\") \\\n",
        "        .save()\n",
        "\n",
        "    nameDT.write \\\n",
        "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
        "        .mode('append') \\\n",
        "        .options(table=\"name\", keyspace=\"movielen\") \\\n",
        "        .save()\n",
        "\n",
        "    # Read DataFrames from Cassandra\n",
        "    readUser = spark.read \\\n",
        "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
        "        .options(table=\"user\", keyspace=\"movielen\").load()\n",
        "\n",
        "    readRating = spark.read \\\n",
        "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
        "        .options(table=\"rating\", keyspace=\"movielen\").load()\n",
        "\n",
        "    readName = spark.read \\\n",
        "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
        "        .options(table=\"name\", keyspace=\"movielen\").load()\n",
        "\n",
        "    # Create temporary views for DataFrames\n",
        "    readUser.createOrReplaceTempView(\"user\")\n",
        "    readRating.createOrReplaceTempView(\"rating\")\n",
        "    readName.createOrReplaceTempView(\"name\")\n",
        "\n",
        "    # i) Calculate the average rating for each movie\n",
        "    avg_ratings = readRating.groupBy(\"movie_id\").agg(avg(\"rating\").alias(\"avg_rating\")).orderBy(col(\"avg_rating\").desc())\n",
        "    avg_ratings.show(10)\n",
        "\n",
        "    # ii) Identify the top ten movies with the highest average ratings\n",
        "    top_movies = avg_ratings.join(readName, \"movie_id\").select(\"title\", \"avg_rating\").orderBy(col(\"avg_rating\").desc()).limit(10)\n",
        "    top_movies.show()\n",
        "\n",
        "    # iii) Find the users who have rated at least 50 movies and identify their favourite movie genres\n",
        "    user_ratings_count = readRating.groupBy(\"user_id\").count().filter(col(\"count\") >= 50)\n",
        "    user_genre_ratings = readRating.join(readName, \"movie_id\").withColumn(\"genre\", explode(array([col(f).alias(f) for f in [\n",
        "        \"unknown\", \"action\", \"adventure\", \"animation\", \"children\", \"comedy\", \"crime\", \"documentary\",\n",
        "        \"drama\", \"fantasy\", \"film_noir\", \"horror\", \"musical\", \"mystery\", \"romance\", \"sci_fi\", \"thriller\", \"war\", \"western\"]])))\n",
        "    user_genre_ratings = user_genre_ratings.groupBy(\"user_id\", \"genre\").count()\n",
        "    frequent_users_genres = user_ratings_count.join(user_genre_ratings, \"user_id\").orderBy(\"user_id\", \"count\", ascending=[1, 0])\n",
        "    frequent_users_genres.show(10)\n",
        "\n",
        "    # iv) Find all the users with age that is less than 20 years old\n",
        "    young_users = readUser.filter(col(\"age\") < 20)\n",
        "    young_users.show(10)\n",
        "\n",
        "    # v) Find all the users who have the occupation “scientist” and their age is between 30 and 40 years old\n",
        "    scientists = readUser.filter((col(\"occupation\") == \"scientist\") & (col(\"age\").between(30, 40)))\n",
        "    scientists.show(10)\n",
        "\n",
        "\n",
        "    spark.stop()"
      ],
      "metadata": {
        "id": "anQA8AH8gMXd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}